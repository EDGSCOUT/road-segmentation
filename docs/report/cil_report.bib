@Article{kallet04methods,
  author = 	 {Richard H Kallet},
  title = 	 {How to Write the Methods Section of a Research Paper},
  journal = 	 {Respiratory Care},
  year = 	 2004,
  volume =	 49,
  number =	 10,
  pages =	 {1229--1232}
}

@Unpublished{anderson04,
  author = 		 {Greg Anderson},
  title = 		 {How to Write a Paper in Scientific Journal Style and Format},
  year = 	 2004,
  organization = {Bates College},
  note = {http://abacus.bates.edu/~ganderso/biology/resources/writing/HTWtoc.html}
}

@Unpublished{jones08,
  author = 		 {Simon Peyton Jones},
  title = 		 {How to write a great research paper},
  note = 		 {Microsoft Research Cambridge},
  year = 	 2008}

@Article{editor10,
  author = 		 {Editorial},
  title = 		 {Scientific writing 101},
  journal = 	 {Nature Structural \& Molecular Biology},
  year = 		 2010,
  volume = 	 17,
  pages = 	 139}

@TechReport{wavelab,
  author = 		 {Jonathan B. Buckheit and David L. Donoho},
  title = 		 {WaveLab and Reproducible Research},
  institution =  {Stanford University},
  year = 		 2009}

@article{gentleman05,
        title = {Reproducible Research: A Bioinformatics Case Study},
        author = {Gentleman, Robert},
        year = {2005},
        journal = {Statistical Applications in Genetics and Molecular Biology},
        volume = 4,
        number = 1,
        publisher = {The Berkeley Electronic Press},
        url = {http://www.bepress.com/sagmb/vol4/iss1/art2}
}

@article{schwab00,
 author = {Schwab, Matthias and Karrenbach, Martin and Claerbout, Jon},
 title = {Making scientific computations reproducible},
 journal = {Computing in Science and Engg.},
 volume = {2},
 number = {6},
 year = {2000},
 issn = {1521-9615},
 pages = {61--67},
 doi = {http://dx.doi.org/10.1109/5992.881708},
 publisher = {IEEE Educational Activities Department},
 address = {Piscataway, NJ, USA},
 }
 
 @Unpublished{mato,
 author = {Delio Vicini, Majet Hamas, Taivo Pungas},
 title = {Road Extraction from Aerial Images},
 note = {https://github.com/mato93/road-extraction-from-aerial-images}
 }
 
@article{VincentPASCALVINCENT2010,
abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpass-ing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordi-nary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{Vincent PASCALVINCENT}, Pascal and {Larochelle LAROCHEH}, Hugo},
doi = {10.1111/1467-8535.00290},
eprint = {0-387-31073-8},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {autoencoders,deep belief networks,deep learning,denoising,unsupervised feature learning},
pmid = {17348934},
title = {{Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion Pierre-Antoine Manzagol}},
year = {2010}
}

@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\{}{\%}{\}} and 17.0 {\{}{\%}{\}} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\{}{\%}{\}}, compared to 26.2 {\{}{\%}{\}} achieved by the second-best entry. 1},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and {Geoffrey E.}, Hinton},
doi = {10.1109/5.726791},
eprint = {1102.0183},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 25 (NIPS2012)},
pages = {1--9},
pmid = {15823584},
title = {{Imagenet}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}

@inproceedings{Eigen2016,
abstract = {In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.},
archivePrefix = {arXiv},
arxivId = {1411.4734},
author = {Eigen, David and Fergus, Rob},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.304},
eprint = {1411.4734},
isbn = {9781467383912},
issn = {15505499},
pages = {2650--2658},
title = {{Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture}},
volume = {11-18-December-2015},
year = {2016}
}

@article{DBLP:journals/corr/BadrinarayananK15,
  author    = {Vijay Badrinarayanan and
               Alex Kendall and
               Roberto Cipolla},
  title     = {SegNet: {A} Deep Convolutional Encoder-Decoder Architecture for Image
               Segmentation},
  journal   = {CoRR},
  volume    = {abs/1511.00561},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.00561},
  timestamp = {Wed, 07 Jun 2017 14:40:10 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BadrinarayananK15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/MaoSY16a,
  author    = {Xiao{-}Jiao Mao and
               Chunhua Shen and
               Yu{-}Bin Yang},
  title     = {Image Restoration Using Convolutional Auto-encoders with Symmetric
               Skip Connections},
  journal   = {CoRR},
  volume    = {abs/1606.08921},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.08921},
  timestamp = {Wed, 07 Jun 2017 14:42:34 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MaoSY16a},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@Book{spolsky04software,
  author = 	 {Joel Spolsky},
  title = 		 {Joel on Software: And on Diverse \& Occasionally Related Matters That Will Prove of Interest etc..: And on Diverse and Occasionally Related Matters ... or Ill-Luck, Work with Them in Some Capacity},
  publisher = 	 {APRESS},
  year = 		 2004}

@Book{hunt99pragmatic,
  author = 	 {Andrew Hunt and David Thomas},
  title = 		 {The Pragmatic Programmer},
  publisher = 	 {Addison Wesley},
  year = 		 1999}

